import streamlit as st
import sqlite3
import pandas as pd
import json
import re
import requests
from Bio import Entrez
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from openai import OpenAI
import time
import db_handler as db  # [ì¤‘ìš”] DB í•¸ë“¤ëŸ¬ ì„í¬íŠ¸

# ===================== [ì•± ì‹œì‘ ì‹œ DB ë™ê¸°í™”] =====================
# ì•±ì´ ì¼œì§ˆ ë•Œ GitHubì—ì„œ ìµœì‹  DB íŒŒì¼ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
if 'db_synced' not in st.session_state:
    try:
        with st.spinner("ë°ì´í„° ë™ê¸°í™” ì¤‘..."):
            db.pull_db()
        st.session_state.db_synced = True
    except Exception as e:
        st.warning(f"DB ë™ê¸°í™” ì‹¤íŒ¨: {e}")

# ===================== [ì„¤ì • ë° ì´ˆê¸°í™”] =====================
st.set_page_config(page_title="í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro", layout="wide", page_icon="ğŸ¥")

with st.sidebar:
    st.header("âš™ï¸ ê¸°ë³¸ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    email_address = st.text_input("Email (PubMedìš©)", value="your_email@example.com")
    
    st.divider()
    st.header("ğŸ“¢ í…”ë ˆê·¸ë¨ ì„¤ì •")
    st.caption("ë‹¨í†¡ë°© IDëŠ” ë³´í†µ ë§ˆì´ë„ˆìŠ¤(-)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    telegram_token = st.text_input("Bot Token", type="password")
    chat_id = st.text_input("Chat ID")

Entrez.email = email_address
DB_NAME = 'kmd_papers_v5_column.db' 

# ===================== [1. DB ê´€ë¦¬] =====================
def init_db():
    # db_handlerì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ, ì•ˆì „ì¥ì¹˜ë¡œ ë‘ 
    pass

def get_papers_by_date(target_date_str):
    conn = sqlite3.connect(DB_NAME)
    try:
        query = "SELECT * FROM papers WHERE date_published = ?"
        df = pd.read_sql(query, conn, params=(target_date_str,))
    except:
        df = pd.DataFrame()
    conn.close()
    return df

def get_daily_column(date_str):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT content FROM daily_columns WHERE date_id = ?", (date_str,))
        result = cursor.fetchone()
        return result[0] if result else None
    except: return None
    finally: conn.close()

def save_daily_column(date_str, content):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO daily_columns VALUES (?, ?, ?)", 
                   (date_str, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit()
    conn.close()
    # [ì¤‘ìš”] ì €ì¥ í›„ GitHub ì—…ë¡œë“œ
    db.push_db()

def get_blog_post(date_str, target_type):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT content FROM blog_posts WHERE date_id = ? AND target_type = ?", (date_str, target_type))
        result = cursor.fetchone()
        return result[0] if result else None
    except: return None
    finally: conn.close()

def save_blog_post(date_str, target_type, content):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO blog_posts VALUES (?, ?, ?, ?)", 
                   (date_str, target_type, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit()
    conn.close()
    # [ì¤‘ìš”] ì €ì¥ í›„ GitHub ì—…ë¡œë“œ
    db.push_db()

def delete_papers(pmid_list):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    if pmid_list:
        placeholders = ', '.join('?' for _ in pmid_list)
        cursor.execute(f"DELETE FROM papers WHERE pmid IN ({placeholders})", pmid_list)
        conn.commit()
    conn.close()
    # [ì¤‘ìš”] ì‚­ì œ í›„ GitHub ì—…ë¡œë“œ
    db.push_db()

def check_if_exists(pmid):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM papers WHERE pmid=?", (pmid,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ===================== [2. Full Text Fetcher] =====================
def fetch_pmc_fulltext(pmid):
    try:
        link_results = Entrez.elink(dbfrom="pubmed", db="pmc", id=pmid)
        if not link_results or not link_results[0]['LinkSetDb']:
            return None, "Abstract Only"
        pmc_id = link_results[0]['LinkSetDb'][0]['Link'][0]['Id']
        handle = Entrez.efetch(db="pmc", id=pmc_id, rettype="xml")
        xml_data = handle.read()
        root = ET.fromstring(xml_data)
        full_text = "".join([text for body in root.findall(".//body") for text in body.itertext()])
        return full_text[:25000] if len(full_text) > 500 else None, "âœ… Full Text (PMC)"
    except Exception as e:
        return None, f"Error: {str(e)}"

# ===================== [3. AI ë¶„ì„ ë¡œì§] =====================
def analyze_paper_strict(paper_data, api_key):
    client = OpenAI(api_key=api_key)
    prompt = f"""
    ë„ˆëŠ” ì„ìƒ í•œì˜í•™ ë…¼ë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€ë‹¤.
    
    [í•„ìˆ˜ ê·œì¹™ 1: ì¤‘ì¬ë²• ë¶„ë¥˜]
    - ì¹¨, ëœ¸, ë¶€í•­, í•œì•½, ì•½ì¹¨, ì¶”ë‚˜ ì¤‘ íƒ1. í•´ë‹¹ ì—†ìœ¼ë©´ "ê¸°íƒ€".

    [í•„ìˆ˜ ê·œì¹™ 2: ì‹ ì²´ë¶€ìœ„ ë¶„ë¥˜]
    - ë‘ê²½ë¶€, ì²™ì¶”/í—ˆë¦¬, ìƒì§€, í•˜ì§€, ë‚´ì¥ê¸°/ì „ì‹  ì¤‘ íƒ1.

    [JSON í˜•ì‹]
    {{
        "korean_title": "í•œê¸€ ì œëª©",
        "study_design": "ì—°êµ¬ ìœ í˜•",
        "intervention_category": "ì¹´í…Œê³ ë¦¬",
        "target_body_part": "ì‹ ì²´ë¶€ìœ„",
        "specific_point": "ìƒì„¸ ì¤‘ì¬ ë‚´ìš©",
        "clinical_score": 8,
        "summary": "3ì¤„ ìš”ì•½",
        "icd_code": "ì½”ë“œ",
        "full_text_status": "Abstract Check"
    }}

    Title: {paper_data['title']}
    Abstract: {paper_data['abstract']}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        data = json.loads(re.search(r'\{.*\}', response.choices[0].message.content.strip(), re.DOTALL).group())
        if "DROP" in str(data.get("study_design", "")): return {"error": "DROP: ì„ìƒ ì—°êµ¬ ì•„ë‹˜"}
        return data
    except Exception as e:
        return {"error": str(e)}

# ===================== [4. PubMed ê²€ìƒ‰] =====================
def simple_keyword_classify(text):
    text = text.lower()
    if "acupuncture" in text or "needling" in text: return "ì¹¨"
    elif "moxibustion" in text: return "ëœ¸"
    elif "cupping" in text: return "ë¶€í•­"
    elif "herbal" in text or "decoction" in text: return "í•œì•½"
    elif "pharmacopuncture" in text or "injection" in text: return "ì•½ì¹¨"
    elif "chuna" in text or "manipulation" in text: return "ì¶”ë‚˜"
    else: return "ê¸°íƒ€"

def search_pubmed_raw(start_date, end_date, max_results):
    str_start = start_date.strftime("%Y/%m/%d")
    str_end = end_date.strftime("%Y/%m/%d")
    search_term = """
    ("TCM" OR "Traditional chinese medicine" OR "Herbal medicine" OR "Medicine, Korean Traditional" 
    OR "Acupuncture" OR "Moxibustion" OR "Cupping Therapy" OR "Pharmacopuncture" OR "Chuna") 
    AND (hasabstract[text]) AND ("Humans"[Mesh]) 
    AND ("Case Reports"[ptyp] OR "Clinical Trial"[ptyp] OR "Randomized Controlled Trial"[ptyp] OR "Systematic Review"[ptyp] OR "Cohort Studies"[Mesh])
    """
    try:
        handle = Entrez.esearch(db="pubmed", term=search_term, mindate=str_start, maxdate=str_end, datetype="pdat", retmax=max_results)
        record = Entrez.read(handle)
        id_list = record["IdList"]
    except: return []

    if not id_list: return []

    try:
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="xml")
        records = Entrez.read(handle)
    except: return []

    raw_papers = []
    for article in records['PubmedArticle']:
        try:
            pmid = str(article['MedlineCitation']['PMID'])
            title = article['MedlineCitation']['Article']['ArticleTitle']
            abstract_list = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
            abstract = " ".join(abstract_list) if abstract_list else ""
            
            raw_papers.append({
                "pmid": pmid,
                "title": title,
                "abstract": abstract,
                "predicted_category": simple_keyword_classify(title + abstract),
                "is_saved": check_if_exists(pmid)
            })
        except: continue
    return raw_papers

# ===================== [5. ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ìƒì„±ê¸°] =====================
def generate_daily_briefing_pro_v3(date_str, papers_df, api_key, model_choice):
    client = OpenAI(api_key=api_key)
    top_papers = papers_df.sort_values(by='clinical_score', ascending=False).head(10)
    
    if top_papers.empty: return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    analyzed_data = []
    prog_bar = st.progress(0)
    status_text = st.empty()

    for idx, (_, row) in enumerate(top_papers.iterrows()):
        prog_bar.progress((idx+1)/len(top_papers))
        status_text.text(f"ğŸ” ì•ˆì „ ë¶„ì„ ëª¨ë“œ ë™ì‘ ì¤‘... ({idx+1}): {row['title_kr']}")
        
        full_text, ft_status = fetch_pmc_fulltext(row['pmid'])
        content_source = full_text if full_text else row['abstract']
        
        pico_prompt = f"""
        ì´ ë…¼ë¬¸ì„ PICO êµ¬ì¡°ë¡œ ë¶„ì„í•˜ë¼.
        [ê·œì¹™] Full Name ë³€í™˜ ë° ìˆ˜ì¹˜ ì •ë³´ í¬í•¨.
        Title: {row['title_kr']}
        Text: {content_source[:15000]}
        """
        try:
            pico_res_text = client.chat.completions.create(
                model="gpt-4o-mini", 
                messages=[{"role": "user", "content": pico_prompt}],
                temperature=0.0
            ).choices[0].message.content
        except: pico_res_text = "ë¶„ì„ ì‹¤íŒ¨"

        analyzed_data.append({
            "pmid": row['pmid'],
            "title": row['title_kr'],
            "score": row['clinical_score'],
            "study_design": row['study_design'],
            "source": ft_status,
            "detail_analysis": pico_res_text
        })

    status_text.empty()
    prog_bar.empty()

    final_prompt = f"""
    ë‹¹ì‹ ì€ í•œì˜í•™ ì—ë””í„°ì…ë‹ˆë‹¤. ìƒìœ„ 7ê°œ(Pick 2 + News 5) ë…¼ë¬¸ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.
    [í•„ìˆ˜] ì›ë¬¸ ë§í¬ í¬í•¨: `ğŸ”— ì›ë¬¸: https://pubmed.ncbi.nlm.nih.gov/[PMID]`
    
    [ì¶œë ¥ í¬ë§·]
    ğŸ“… **{date_str} í•œì˜ ì„ìƒ ë¸Œë¦¬í•‘**
    
    ğŸ¥‡ **Today's Pick 1: [ì œëª©]**
    ([ì—°êµ¬ìœ í˜•] / â­[ì ìˆ˜])
    - ğŸ¯ **Point:** ...
    - ğŸ’Š **Method:** ...
    - ğŸ“Š **Result:** ...
    - ğŸ”— **ì›ë¬¸:** https://pubmed.ncbi.nlm.nih.gov/[PMID]
    
    [ì…ë ¥ ë°ì´í„°]
    {json.dumps(analyzed_data, ensure_ascii=False)}
    """
    
    try:
        if "o1" in model_choice:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": final_prompt}]).choices[0].message.content
        else:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": final_prompt}], temperature=0.7).choices[0].message.content
    except Exception as e: return f"ìƒì„± ì‹¤íŒ¨: {e}"

# ===================== [6. ë¸”ë¡œê·¸ ì•„í‹°í´ ìƒì„±ê¸°] =====================
def generate_blog_article(date_str, papers_df, api_key, model_choice, target_audience="doctor"):
    client = OpenAI(api_key=api_key)
    top_paper = papers_df.sort_values(by='clinical_score', ascending=False).iloc[0]
    full_text, ft_status = fetch_pmc_fulltext(top_paper['pmid'])
    content_source = full_text if full_text else top_paper['abstract']
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ì˜í•™ ë¸”ë¡œê±°ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.
    íƒ€ê²Ÿ: {'ì „ë¬¸ê°€(í•œì˜ì‚¬)' if target_audience == 'doctor' else 'ì¼ë°˜ í™˜ì'}
    
    [ë…¼ë¬¸ ì •ë³´]
    ì œëª©: {top_paper['title_kr']}
    PMID: {top_paper['pmid']}
    ë‚´ìš©: {content_source[:20000]}
    """
    
    try:
        if "o1" in model_choice:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
        else:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": prompt}], temperature=0.7).choices[0].message.content
    except Exception as e: return f"ë¸”ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}"

# ===================== [7. UI êµ¬ì„±] =====================
st.title("ğŸ¥ í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro")
st.markdown("---")

tab_briefing, tab_blog, tab_archive, tab_search = st.tabs(["ğŸ“ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘", "âœï¸ ë¸”ë¡œê·¸/ìˆ˜ìµí™”", "ğŸ“š ë³´ê´€í•¨", "ğŸ” ê²€ìƒ‰"])

# --- [Tab 1: ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ & í…”ë ˆê·¸ë¨] ---
with tab_briefing:
    c1, c2 = st.columns([1, 2])
    with c1:
        st.subheader("ğŸ—“ ë¸Œë¦¬í•‘ ì„¤ì •")
        target_date = st.date_input("ë‚ ì§œ", value=datetime.now())
        target_date_str = target_date.strftime("%Y-%m-%d")
        daily_papers = get_papers_by_date(target_date_str)
        st.info(f"ë…¼ë¬¸ ìˆ˜: {len(daily_papers)}ê±´")
        
        model_option = st.radio("AI ëª¨ë¸:", ["gpt-4o", "o1-preview", "gpt-4o-mini"], index=0)
        
        if st.button("âœ¨ ì•ˆì „ ëª¨ë“œ ë¸Œë¦¬í•‘ ìƒì„±"):
            if daily_papers.empty: st.error("ë…¼ë¬¸ ì—†ìŒ")
            elif not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                briefing = generate_daily_briefing_pro_v3(target_date_str, daily_papers, openai_api_key, model_option)
                save_daily_column(target_date_str, briefing)
                st.success("ë¸Œë¦¬í•‘ ìƒì„± ë° ì €ì¥ ì™„ë£Œ!") # GitHub ì—…ë¡œë“œ ë¨
                st.rerun()

    with c2:
        st.subheader("ğŸ“¨ ê³µìœ  ë° ì „ì†¡")
        content = get_daily_column(target_date_str)
        
        if content:
            st.markdown("##### ğŸš€ í…”ë ˆê·¸ë¨ ì „ì†¡")
            user_footer = st.text_area("ğŸ“¢ ì¶”ê°€ ì½”ë©˜íŠ¸", height=70)
            final_msg = content
            if user_footer: final_msg += f"\n\n--------------------------------\nğŸ“¢ **Editor's Note**\n{user_footer}"

            if st.button("âœˆï¸ í…”ë ˆê·¸ë¨ ì „ì†¡", type="primary"):
                if not telegram_token or not chat_id:
                    st.error("ì„¤ì •ì—ì„œ í† í°/IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                else:
                    try:
                        url = f"https://api.telegram.org/bot{telegram_token}/sendMessage"
                        res = requests.post(url, json={"chat_id": chat_id, "text": final_msg, "parse_mode": "Markdown"})
                        if res.status_code == 200: st.toast("ì „ì†¡ ì„±ê³µ!"); st.success("ì „ì†¡ ì™„ë£Œ")
                        else: st.error(f"ì‹¤íŒ¨: {res.text}")
                    except Exception as e: st.error(f"ì—ëŸ¬: {e}")

            st.divider()
            st.code(final_msg, language='markdown')
            with st.expander("ë¯¸ë¦¬ë³´ê¸°"): st.markdown(final_msg)
        else:
            st.warning("ìƒì„±ëœ ë¸Œë¦¬í•‘ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- [Tab 2: ë¸”ë¡œê·¸] ---
with tab_blog:
    c_b1, c_b2 = st.columns([1, 3])
    with c_b1:
        st.subheader("âœ’ï¸ ë¸”ë¡œê·¸ ìƒì„±")
        b_date = st.date_input("ë‚ ì§œ", value=datetime.now(), key="blog_date")
        b_date_str = b_date.strftime("%Y-%m-%d")
        b_papers = get_papers_by_date(b_date_str)
        st.info(f"í›„ë³´: {len(b_papers)}ê±´")
        b_model = st.selectbox("ëª¨ë¸:", ["gpt-4o", "o1-preview"], index=0)
        target_type = st.radio("íƒ€ê²Ÿ:", ["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš©", "ğŸ˜Š í™˜ììš©"])
        
        if st.button("âœï¸ ê¸€ ìƒì„±"):
            if b_papers.empty: st.error("ë…¼ë¬¸ ì—†ìŒ")
            elif not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                with st.spinner("ì‘ì„± ì¤‘..."):
                    article = generate_blog_article(b_date_str, b_papers, openai_api_key, b_model, "doctor" if "ì „ë¬¸ê°€" in target_type else "patient")
                    save_blog_post(b_date_str, "doctor" if "ì „ë¬¸ê°€" in target_type else "patient", article)
                    st.success("ì™„ë£Œ! (GitHub ìë™ ì €ì¥ë¨)")
                    st.rerun()

    with c_b2:
        st.subheader("ğŸ“„ ë¯¸ë¦¬ë³´ê¸°")
        t1, t2 = st.tabs(["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš©", "ğŸ˜Š í™˜ììš©"])
        with t1:
            post = get_blog_post(b_date_str, "doctor")
            if post: st.markdown(post); st.divider(); st.code(post)
            else: st.info("ì—†ìŒ")
        with t2:
            post = get_blog_post(b_date_str, "patient")
            if post: st.markdown(post); st.divider(); st.code(post)
            else: st.info("ì—†ìŒ")

# --- [Tab 3: ë³´ê´€í•¨ (í•„í„° ì ìš©)] ---
with tab_archive:
    # 1. DBì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_all = pd.read_sql("SELECT * FROM papers", sqlite3.connect(DB_NAME))
    
    if df_all.empty:
        st.info("ë³´ê´€í•¨ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    else:
        st.subheader("ğŸ” í•„í„°ë§")
        
        # 2. í•„í„° UI
        cats = sorted(df_all['intervention_category'].unique().tolist())
        sel_cats = st.multiselect("ì¤‘ì¬ë²• ì„ íƒ", cats, default=cats)
        
        if 'archive_body_part' not in st.session_state: st.session_state.archive_body_part = "ì „ì²´"
        def btn_col(part): return "primary" if st.session_state.archive_body_part == part else "secondary"
        
        parts = ["ë‘ê²½ë¶€", "ì²™ì¶”/í—ˆë¦¬", "ìƒì§€", "í•˜ì§€", "ë‚´ì¥ê¸°/ì „ì‹ ", "ì „ì²´"]
        cols = st.columns(6)
        for i, part in enumerate(parts):
            if cols[i].button(part, key=f"p_{i}", type=btn_col(part), use_container_width=True):
                st.session_state.archive_body_part = part
                st.rerun()

        # 3. ë°ì´í„° í•„í„°ë§
        df_filt = df_all.copy()
        if sel_cats: df_filt = df_filt[df_filt['intervention_category'].isin(sel_cats)]
        if st.session_state.archive_body_part != "ì „ì²´":
            df_filt = df_filt[df_filt['target_body_part'] == st.session_state.archive_body_part]

        st.divider()
        st.subheader(f"ğŸ“š ëª©ë¡ ({len(df_filt)}ê±´)")
        
        if not df_filt.empty:
            # ì‚­ì œ ì²´í¬ë°•ìŠ¤ ë° URL ë§í¬ ìƒì„±
            df_filt.insert(0, "del", False)
            df_filt["url"] = "https://pubmed.ncbi.nlm.nih.gov/" + df_filt["pmid"]
            
            # 4. ë°ì´í„° ì—ë””í„° í‘œì‹œ (ì—¬ê¸°ì— 'date_published' ì¶”ê°€ë¨!)
            edited = st.data_editor(
                df_filt,
                column_config={
                    "del": st.column_config.CheckboxColumn("ì‚­ì œ", width="small"),
                    "url": st.column_config.LinkColumn("Link", display_text="ğŸ”—", width="small"),
                    "date_published": st.column_config.TextColumn("ìˆ˜ì§‘ì¼", width="small"), # [ë³µêµ¬ë¨]
                    "title_kr": st.column_config.TextColumn("ì œëª©", width="large"),
                    "target_body_part": st.column_config.TextColumn("ë¶€ìœ„", width="small"),
                    "intervention_category": st.column_config.TextColumn("ì¤‘ì¬", width="small"),
                    "clinical_score": st.column_config.NumberColumn("ì ìˆ˜", format="%dì "),
                },
                # ì»¬ëŸ¼ ìˆœì„œ ì§€ì • (ìˆ˜ì§‘ì¼ì„ ì•ìª½ìœ¼ë¡œ ë°°ì¹˜)
                column_order=["del", "url", "date_published", "clinical_score", "intervention_category", "target_body_part", "title_kr", "summary"],
                hide_index=True, 
                use_container_width=True
            )
            
            # 5. ì‚­ì œ ë¡œì§
            if st.button("ğŸ—‘ï¸ ì‚­ì œ í™•ì¸"):
                to_del = edited[edited["del"]]['pmid'].tolist()
                if to_del:
                    delete_papers(to_del)
                    st.success("ì‚­ì œë¨ (GitHub ìë™ ë™ê¸°í™”)")
                    st.rerun()
        else: st.warning("ì¡°ê±´ì— ë§ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- [Tab 4: ê²€ìƒ‰] ---
with tab_search:
    c1, c2 = st.columns(2)
    with c1: s_date = st.date_input("ì‹œì‘", value=datetime.now()-timedelta(days=2))
    with c2: e_date = st.date_input("ì¢…ë£Œ", value=datetime.now())
    limit = st.slider("ê°œìˆ˜", 10, 100, 50)
    
    if 'search_res' not in st.session_state: st.session_state.search_res = None
    if st.button("1. ê²€ìƒ‰"):
        with st.spinner(".."): st.session_state.search_res = search_pubmed_raw(s_date, e_date, limit)
        
    if st.session_state.search_res:
        df = pd.DataFrame(st.session_state.search_res)
        df.insert(0, "Sel", ~df['is_saved'])
        edited = st.data_editor(df, column_config={"Sel": st.column_config.CheckboxColumn("ì„ íƒ")}, hide_index=True)
        targets = edited[edited["Sel"]]
        
        if st.button(f"2. {len(targets)}ê±´ ë¶„ì„ ë° ì €ì¥"):
            if not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                conn = sqlite3.connect(DB_NAME); cur = conn.cursor()
                bar = st.progress(0)
                full_list = [p for p in st.session_state.search_res if p['pmid'] in targets['pmid'].tolist()]
                
                # DB í…Œì´ë¸” ìƒì„± ì²´í¬
                cur.execute('''CREATE TABLE IF NOT EXISTS papers (pmid TEXT PRIMARY KEY, date_published TEXT, title_kr TEXT, intervention_category TEXT, target_body_part TEXT, specific_point TEXT, study_design TEXT, clinical_score INTEGER, summary TEXT, original_title TEXT, abstract TEXT, icd_code TEXT, full_text_status TEXT)''')
                
                for i, p in enumerate(full_list):
                    bar.progress((i+1)/len(full_list))
                    res = analyze_paper_strict(p, openai_api_key)
                    if "error" not in res:
                        cur.execute('INSERT OR REPLACE INTO papers VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', (
                            p['pmid'], datetime.now().strftime('%Y-%m-%d'),
                            res.get('korean_title'), res.get('intervention_category'),
                            res.get('target_body_part'), res.get('specific_point'),
                            res.get('study_design'), res.get('clinical_score'),
                            res.get('summary'), p['title'], p['abstract'], 
                            res.get('icd_code'), "Abstract Saved"
                        ))
                        conn.commit()
                conn.close()
                # [ì¤‘ìš”] ì €ì¥ ì™„ë£Œ í›„ GitHub ì—…ë¡œë“œ
                db.push_db()
                
                st.success("ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ! (GitHub ë™ê¸°í™” ë¨)")
                st.session_state.search_res = None
                time.sleep(1)
                st.rerun()

# ì•± ì‹¤í–‰ ì‹œ DB ì²´í¬ (ì„í¬íŠ¸ ì‹œ ì‹¤í–‰ë˜ì§€ë§Œ ì•ˆì „ìƒ í•œ ë²ˆ ë”)
if __name__ == "__main__":
    if not st.session_state.get('db_synced'):
        db.pull_db()
        st.session_state.db_synced = True


