import streamlit as st
import sqlite3
import pandas as pd
import json
import re
import requests
import urllib.parse
from Bio import Entrez
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from openai import OpenAI
import time
import PyPDF2
import db_handler as db

# ===================== [0. ì´ˆê¸° ì„¤ì •] =====================
st.set_page_config(page_title="í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro", layout="wide", page_icon="ğŸ¥")

if 'db_synced' not in st.session_state:
    try:
        with st.spinner("ë°ì´í„° ë™ê¸°í™” ì¤‘..."):
            db.pull_db()
        st.session_state.db_synced = True
    except Exception as e:
        st.warning(f"DB ë™ê¸°í™” ì‹¤íŒ¨: {e}")

# ===================== [1. ë¬´ë£Œ íŒŒì´ì¬ ë¶„ì„ê¸°] =====================
def analyze_metadata_free(text, title):
    text_lower = text.lower() if text else ""
    meta = {"n_count": "", "p_value": "", "tags": []}

    n_match = re.search(r'\bn\s*=\s*(\d+)', text_lower)
    if n_match: meta["n_count"] = f"n={n_match.group(1)}"

    if "p<0.05" in text_lower.replace(" ", "") or "p < 0.05" in text_lower:
        meta["p_value"] = "âœ… P<0.05"

    if "acupotomy" in text_lower or "miniscalpel" in text_lower: meta["tags"].append("#ë„ì¹¨")
    if "pharmacopuncture" in text_lower or "bee venom" in text_lower: meta["tags"].append("#ì•½ì¹¨")
    if "chuna" in text_lower or "tuina" in text_lower: meta["tags"].append("#ì¶”ë‚˜")
    if "thread" in text_lower and "embedding" in text_lower: meta["tags"].append("#ë§¤ì„ ")
    if "herbal" in text_lower or "decoction" in text_lower: meta["tags"].append("#í•œì•½")
    
    meta["tags_str"] = ", ".join(meta["tags"])
    return meta

# ===================== [2. NEW: ì¶”ê°€ëœ ì‹¬ì¸µ ë¶„ì„ ë„êµ¬ë“¤] =====================
def read_pdf_file(uploaded_file):
    try:
        reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text[:30000]
    except: return None

# [ì—…ê·¸ë ˆì´ë“œ] ë§í¬ ìƒì„±ì„ ìœ„í•´ URL ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •ë¨
def get_consensus_evidence(topic_query, required_keywords=[]):
    """
    ì£¼ì œ(Query)ë¡œ ê²€ìƒ‰í•˜ë˜, ê²°ê³¼ ë…¼ë¬¸ ì œëª©ì— required_keywords(í•µì‹¬ë‹¨ì–´)ê°€ 
    í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ 'ê°€ì§œ ê²°ê³¼'ë¡œ ê°„ì£¼í•˜ê³  ë²„ë¦½ë‹ˆë‹¤.
    """
    try:
        # ê²€ìƒ‰ì–´: (ì£¼ì œ) AND (RCT/Review) AND (ìµœì‹  5ë…„)
        search_term = f"({topic_query}) AND (Systematic Review[ptyp] OR Meta-Analysis[ptyp] OR Randomized Controlled Trial[ptyp]) AND (\"2015\"[Date - Publication] : \"3000\"[Date - Publication])"
        
        handle = Entrez.esearch(db="pubmed", term=search_term, retmax=10, sort="relevance") # 10ê°œ ë„‰ë„‰íˆ ê°€ì ¸ì˜´
        record = Entrez.read(handle)
        id_list = record["IdList"]
        
        if not id_list: return "ê´€ë ¨ëœ ì¶”ê°€ ê·¼ê±° ë…¼ë¬¸ì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []

        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="xml")
        records = Entrez.read(handle)
        
        evidence_text = ""
        ref_list = []
        valid_count = 0
        
        for article in records['PubmedArticle']:
            try:
                title = article['MedlineCitation']['Article']['ArticleTitle']
                abstract_list = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
                abstract = " ".join(abstract_list) if abstract_list else ""
                
                # [ğŸ›¡ï¸ í•µì‹¬: ì•ˆì „ì¥ì¹˜] ì œëª©ì— í•µì‹¬ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ ê²€ì‚¬
                # ì˜ˆ: required_keywords = ['allergic', 'rhinitis']
                # ì œëª©ì´ "Kidney disease..." ì´ë©´ -> íƒˆë½!
                is_relevant = False
                if not required_keywords: # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê·¸ëƒ¥ í†µê³¼
                    is_relevant = True
                else:
                    for keyword in required_keywords:
                        if keyword.lower() in title.lower():
                            is_relevant = True
                            break
                
                if not is_relevant:
                    continue # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ ë…¼ë¬¸ ë´„

                # ê²€ì¦ í†µê³¼í•œ ë…¼ë¬¸ë§Œ ì¶”ê°€
                valid_count += 1
                evidence_text += f"\n[Ref {valid_count}] {title}\nìš”ì•½: {abstract[:200]}...\n"
                
                pmid = str(article['MedlineCitation']['PMID'])
                ref_list.append({
                    "index": valid_count,
                    "title": title,
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}"
                })
                
                if valid_count >= 5: break # 5ê°œ ì±„ìš°ë©´ ì¢…ë£Œ
                
            except: continue
            
        if valid_count == 0:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì—ˆìœ¼ë‚˜, ì£¼ì œì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë…¼ë¬¸ì€ ì—†ì—ˆìŠµë‹ˆë‹¤.", []
            
        return evidence_text, ref_list
        
    except Exception as e: return f"êµì°¨ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

# ===================== [UI ì‚¬ì´ë“œë°”] =====================
with st.sidebar:
    st.header("âš™ï¸ ê¸°ë³¸ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    email_address = st.text_input("Email (PubMedìš©)", value="your_email@example.com")
    st.divider()
    telegram_token = st.text_input("Bot Token", type="password")
    chat_id = st.text_input("Chat ID")

Entrez.email = email_address
DB_NAME = 'kmd_papers_v5_column.db' 

# ===================== [3. DB ê´€ë¦¬ & ë§ˆì´ê·¸ë ˆì´ì…˜] =====================
def migrate_db():
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    new_columns = [("n_count", "TEXT"), ("p_value", "TEXT"), ("tags", "TEXT"), ("user_note", "TEXT")]
    for col, dtype in new_columns:
        try: cursor.execute(f"ALTER TABLE papers ADD COLUMN {col} {dtype}")
        except sqlite3.OperationalError: pass
    conn.commit(); conn.close()

def get_papers_by_date(target_date_str):
    conn = sqlite3.connect(DB_NAME)
    try:
        query = "SELECT * FROM papers WHERE date_published = ?"
        df = pd.read_sql(query, conn, params=(target_date_str,))
    except: df = pd.DataFrame()
    conn.close()
    return df

def get_daily_column(date_str):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    try:
        cursor.execute("SELECT content FROM daily_columns WHERE date_id = ?", (date_str,))
        res = cursor.fetchone()
        return res[0] if res else None
    except: return None
    finally: conn.close()

def save_daily_column(date_str, content):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO daily_columns VALUES (?, ?, ?)", (date_str, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit(); conn.close(); db.push_db()

def get_blog_post(date_str, target_type):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    try:
        cursor.execute("SELECT content FROM blog_posts WHERE date_id = ? AND target_type = ?", (date_str, target_type))
        res = cursor.fetchone()
        return res[0] if res else None
    except: return None
    finally: conn.close()

def save_blog_post(date_str, target_type, content):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO blog_posts VALUES (?, ?, ?, ?)", (date_str, target_type, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit(); conn.close(); db.push_db()

def delete_papers(pmid_list):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    if pmid_list:
        placeholders = ', '.join('?' for _ in pmid_list)
        cursor.execute(f"DELETE FROM papers WHERE pmid IN ({placeholders})", pmid_list)
        conn.commit()
    conn.close(); db.push_db()

def check_if_exists(pmid):
    conn = sqlite3.connect(DB_NAME); cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM papers WHERE pmid=?", (pmid,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ===================== [4. Full Text Fetcher] =====================
def fetch_pmc_fulltext(pmid):
    try:
        link_results = Entrez.elink(dbfrom="pubmed", db="pmc", id=pmid)
        if not link_results or not link_results[0]['LinkSetDb']: return None, "Abstract Only"
        pmc_id = link_results[0]['LinkSetDb'][0]['Link'][0]['Id']
        handle = Entrez.efetch(db="pmc", id=pmc_id, rettype="xml")
        xml_data = handle.read()
        root = ET.fromstring(xml_data)
        full_text = "".join([text for body in root.findall(".//body") for text in body.itertext()])
        return full_text[:30000], "âœ… PMC ì „ë¬¸(Full Text) ë¶„ì„ë¨"
    except Exception as e: return None, f"Error: {str(e)}"

# ===================== [5. AI ë¶„ì„ ë¡œì§] =====================
def analyze_paper_strict(paper_data, api_key):
    client = OpenAI(api_key=api_key)
    prompt = f"""
    ë„ˆëŠ” ì„ìƒ í•œì˜í•™ ë…¼ë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€ë‹¤.
    [í•„ìˆ˜ ê·œì¹™] ì¤‘ì¬ë²•(ì¹¨/ëœ¸/ë¶€í•­/í•œì•½/ì•½ì¹¨/ì¶”ë‚˜/ê¸°íƒ€), ë¶€ìœ„(ë‘ê²½ë¶€/ì²™ì¶”/ìƒì§€/í•˜ì§€/ë‚´ì¥ê¸°)
    [JSON í˜•ì‹] {{
        "korean_title": "í•œê¸€ ì œëª©", "study_design": "ì—°êµ¬ ìœ í˜•", "intervention_category": "ì¹´í…Œê³ ë¦¬",
        "target_body_part": "ì‹ ì²´ë¶€ìœ„", "specific_point": "ìƒì„¸ ì¤‘ì¬ ë‚´ìš©", "clinical_score": 8,
        "summary": "3ì¤„ ìš”ì•½", "icd_code": "ì½”ë“œ", "full_text_status": "Abstract Check"
    }}
    Title: {paper_data['title']}
    Abstract: {paper_data['abstract']}
    """
    try:
        response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0.0)
        data = json.loads(re.search(r'\{.*\}', response.choices[0].message.content.strip(), re.DOTALL).group())
        if "DROP" in str(data.get("study_design", "")): return {"error": "DROP"}
        return data
    except Exception as e: return {"error": str(e)}

# ===================== [6. PubMed ê²€ìƒ‰] =====================
def simple_keyword_classify(text):
    text = text.lower()
    if "acupuncture" in text or "needling" in text: return "ì¹¨"
    elif "moxibustion" in text: return "ëœ¸"
    elif "cupping" in text: return "ë¶€í•­"
    elif "herbal" in text or "decoction" in text: return "í•œì•½"
    elif "pharmacopuncture" in text or "injection" in text: return "ì•½ì¹¨"
    elif "chuna" in text or "manipulation" in text: return "ì¶”ë‚˜"
    else: return "ê¸°íƒ€"

def search_pubmed_raw(start_date, end_date, max_results):
    str_start = start_date.strftime("%Y/%m/%d")
    str_end = end_date.strftime("%Y/%m/%d")
    search_term = """
    ("TCM" OR "Traditional chinese medicine" OR "Herbal medicine" OR "Medicine, Korean Traditional" 
    OR "Acupuncture" OR "Moxibustion" OR "Cupping Therapy" OR "Pharmacopuncture" OR "Chuna") 
    AND (hasabstract[text]) AND ("Humans"[Mesh]) 
    AND ("Case Reports"[ptyp] OR "Clinical Trial"[ptyp] OR "Randomized Controlled Trial"[ptyp] OR "Systematic Review"[ptyp] OR "Cohort Studies"[Mesh])
    """
    try:
        handle = Entrez.esearch(db="pubmed", term=search_term, mindate=str_start, maxdate=str_end, datetype="pdat", retmax=max_results)
        id_list = Entrez.read(handle)["IdList"]
        if not id_list: return []
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="xml")
        records = Entrez.read(handle)
    except: return []

    raw_papers = []
    for article in records['PubmedArticle']:
        try:
            pmid = str(article['MedlineCitation']['PMID'])
            title = article['MedlineCitation']['Article']['ArticleTitle']
            abstract_list = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
            abstract = " ".join(abstract_list) if abstract_list else ""
            raw_papers.append({
                "pmid": pmid, "title": title, "abstract": abstract,
                "predicted_category": simple_keyword_classify(title + abstract),
                "is_saved": check_if_exists(pmid)
            })
        except: continue
    return raw_papers

# ===================== [7. ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ìƒì„±ê¸°] =====================
def generate_daily_briefing_pro_v3(date_str, papers_df, api_key, model_choice):
    client = OpenAI(api_key=api_key)
    top_papers = papers_df.sort_values(by='clinical_score', ascending=False).head(10)
    if top_papers.empty: return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    analyzed_data = []
    prog_bar = st.progress(0); status_text = st.empty()

    for idx, (_, row) in enumerate(top_papers.iterrows()):
        prog_bar.progress((idx+1)/len(top_papers))
        status_text.text(f"ğŸ” ì•ˆì „ ë¶„ì„ ëª¨ë“œ ë™ì‘ ì¤‘... ({idx+1}): {row['title_kr']}")
        
        full_text, ft_status = fetch_pmc_fulltext(row['pmid'])
        content_source = full_text if full_text else row['abstract']
        
        pico_prompt = f"""ì´ ë…¼ë¬¸ì„ PICO êµ¬ì¡°ë¡œ ë¶„ì„í•˜ë¼. Title: {row['title_kr']} Text: {content_source[:15000]}"""
        try: pico_res_text = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": pico_prompt}], temperature=0.0).choices[0].message.content
        except: pico_res_text = "ë¶„ì„ ì‹¤íŒ¨"

        analyzed_data.append({
            "pmid": row['pmid'], "title": row['title_kr'], "score": row['clinical_score'],
            "study_design": row['study_design'], "source": ft_status, "detail_analysis": pico_res_text
        })
    status_text.empty(); prog_bar.empty()

    final_prompt = f"""
    [ì—­í• ]
    ë‹¹ì‹ ì€ 20ë…„ ì°¨ ì„ìƒ í•œì˜ì‚¬ì´ì, ìµœì‹  ì˜í•™ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ëŠ” ìˆ˜ì„ ì—ë””í„°ì…ë‹ˆë‹¤.
    ë™ë£Œ í•œì˜ì‚¬ë“¤ì—ê²Œ ë§¤ì¼ ì•„ì¹¨ ì˜ì–‘ê°€ ë†’ì€ ìµœì‹  ë…¼ë¬¸ ì •ë³´ë¥¼ ë¸Œë¦¬í•‘í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.

    [ì‘ì„± ëª©í‘œ]
    ì œê³µëœ JSON ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ë…ì„± ì¢‹ê³ , ì„ìƒì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ **'Daily Clinical Briefing'**ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    [ì…ë ¥ ë°ì´í„°]
    {json.dumps(analyzed_data, ensure_ascii=False)}

    [ì‘ì„± ê°€ì´ë“œë¼ì¸]
    1. **í†¤ì•¤ë§¤ë„ˆ:** ì „ë¬¸ì ì´ì§€ë§Œ ë”±ë”±í•˜ì§€ ì•Šê²Œ. "í•©ë‹ˆë‹¤/í–ˆìŠµë‹ˆë‹¤" ë³´ë‹¤ëŠ” **"~ì„/í•¨/í™•ì¸ë¨"** ë“±ì˜ ê°œì¡°ì‹ í˜¹ì€ **"~í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤"** ì‹ì˜ ì œì•ˆí˜• ì–´ì¡° í˜¼ìš©.
    2. **í•µì‹¬ ê°•ì¡°:** ì¤‘ìš”í•œ ìˆ˜ì¹˜(Nìˆ˜, Pê°’, íš¨ê³¼ í¬ê¸°)ë‚˜ ê²°ë¡ ì€ **êµµì€ ê¸€ì”¨**ë¡œ ê°•ì¡°.
    3. **ë¹„íŒì  ì‹œê°:** ì—°êµ¬ ë””ìì¸(RCT, SR ë“±)ê³¼ ì„ìƒ ì ìˆ˜(score)ë¥¼ ì°¸ê³ í•˜ì—¬, ì‹ ë¢°ë„ê°€ ë‚®ì€ ë…¼ë¬¸ì€ "ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•¨"ì´ë¼ê³  ì–¸ê¸‰.
    4. Huzhangê³¼ ê°™ì´ ë‹¤ë¥¸ë‚˜ë¼ë§ì— ê¸°ë°˜í•œ ë§ì„ í•œêµ­ì–´ë¡œ ë­”ì§€ ëª¨ë¥´ëŠ”ê²ƒì€ ë°˜ë“œì‹œ ì°¾ì•„ì„œ ê´„í˜¸ì•ˆì— ì£¼ì„ì„ ë‹¬ì•„ì¤„ê²ƒ. ì°¾ì§€ëª»í•˜ì˜€ë‹¤ë©´ ì˜¤ëŠ˜ì˜ ë…¼ë¬¸ìœ¼ë¡œ ì‹£ëŠ” ê²ƒì„ í¬ê¸°í•  ê²ƒ
    5. EA, MAì™€ ê°™ì€ ì¤„ì„ë§ì„ ì‚¬ìš©í•œê²ƒì€ EA(electroacupuncture)ë“±ê³¼ ê°™ì´ ë°˜ë“œì‹œ full nameì„ ì ì–´ì¤„ê²ƒ. ì°¾ì§€ëª»í•˜ì˜€ë‹¤ë©´ ì˜¤ëŠ˜ì˜ ë…¼ë¬¸ìœ¼ë¡œ ì‹£ëŠ” ê²ƒì„ í¬ê¸°í•  ê²ƒ

    [ì¶œë ¥ í¬ë§· (Markdown)]
    
    # ğŸ“… {date_str} í•œì˜ ì„ìƒ ë¸Œë¦¬í•‘
    
    ---
    ### ğŸ† Today's Pick: ì§‘ì¤‘ íƒêµ¬ (Top 2)
    *ê°€ì¥ ì„ìƒ ê°€ì¹˜ê°€ ë†’ê±°ë‚˜ í¥ë¯¸ë¡œìš´ ë…¼ë¬¸ 2ê°œë¥¼ ì„ ì •í•˜ì—¬ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.*

    #### 1. [í•œê¸€ ë…¼ë¬¸ ì œëª©]
    > **ì—°êµ¬ì„¤ê³„:** [Study Design] | **ì„ìƒì ìˆ˜:** â­[Score]/10
    
    * **ğŸ¯ í•µì‹¬ ìš”ì•½:** (ì—°êµ¬ì˜ ëª©ì ê³¼ ê²°ë¡ ì„ 1ë¬¸ì¥ìœ¼ë¡œ)
    * **ğŸ’Š ì¤‘ì¬/ë°©ë²•:** (ë¬´ì—‡ì„ ì–´ë–»ê²Œ í–ˆëŠ”ì§€? êµ¬ì²´ì ì¸ í˜ˆìë¦¬, ì²˜ë°©ëª…, ìš©ëŸ‰ ë“±)
    * **ğŸ“Š ì£¼ìš” ê²°ê³¼:** (ëŒ€ì¡°êµ° ëŒ€ë¹„ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë³€í™” í¬í•¨)
    * **ğŸ’¡ ì„ìƒ ì œì–¸:** (ì´ ê²°ê³¼ë¥¼ ì‹¤ì œ í•œì˜ì› ì§„ë£Œì‹¤ì—ì„œ ì–´ë–»ê²Œ í•´ì„í•˜ê³  ì ìš©í•´ì•¼ í• ì§€ ì—ë””í„°ì˜ ê´€ì ìœ¼ë¡œ ì„œìˆ . ì˜ˆ: "ë§Œì„± ìš”í†µ í™˜ì í‹°ì¹­ ì‹œ ì°¸ê³ í•  ë§Œí•©ë‹ˆë‹¤.")
    * ğŸ”— [ì›ë¬¸ ë³´ê¸°](https://pubmed.ncbi.nlm.nih.gov/[PMID])

    (2ë²ˆ ë…¼ë¬¸ë„ ë™ì¼ í˜•ì‹)

    ---
    ### ğŸ“° Research Shorts: ë†“ì¹˜ë©´ ì•„ì‰¬ìš´ ë‹¨ì‹  (5ì„ )
    *ì œëª©ê³¼ í•µì‹¬ ê²°ê³¼ë§Œ ë¹ ë¥´ê²Œ í›‘ì–´ë´…ë‹ˆë‹¤.*

    1. **[í•œê¸€ ì œëª©]** ([Study Design])
       - ê²°ê³¼: í•µì‹¬ ê²°ê³¼ 3ì¤„ ìš”ì•½
       - ğŸ”— [Link](https://pubmed.ncbi.nlm.nih.gov/[PMID])
    
    2. (ë‚˜ë¨¸ì§€ ë…¼ë¬¸ë“¤...)

    ---
   (Data source: PubMed)_
    """
    try: return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": final_prompt}]).choices[0].message.content
    except Exception as e: return f"ìƒì„± ì‹¤íŒ¨: {e}"

# ===================== [9. ë©”ì¸ UI] =====================
migrate_db() # ì•± ì‹¤í–‰ ì‹œ DB êµ¬ì¡° ìë™ ì—…ë°ì´íŠ¸

st.title("ğŸ¥ í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro")
st.markdown("---")

tab_briefing, tab_blog, tab_archive, tab_search = st.tabs(["ğŸ“ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘", "âœï¸ ë¸”ë¡œê·¸/ìˆ˜ìµí™”", "ğŸ“š ë³´ê´€í•¨", "ğŸ” ê²€ìƒ‰"])

# --- [Tab 1: ë°ì¼ë¦¬ ë¸Œë¦¬í•‘] ---
with tab_briefing:
    c1, c2 = st.columns([1, 2])
    with c1:
        st.subheader("ğŸ—“ ë¸Œë¦¬í•‘ ì„¤ì •")
        target_date = st.date_input("ë‚ ì§œ", value=datetime.now())
        target_date_str = target_date.strftime("%Y-%m-%d")
        daily_papers = get_papers_by_date(target_date_str)
        st.info(f"ë…¼ë¬¸ ìˆ˜: {len(daily_papers)}ê±´")
        model_option = st.radio("AI ëª¨ë¸:", ["gpt-4o", "o1-preview", "gpt-4o-mini"], index=0)
        
        if st.button("âœ¨ ì•ˆì „ ëª¨ë“œ ë¸Œë¦¬í•‘ ìƒì„±"):
            if daily_papers.empty: st.error("ë…¼ë¬¸ ì—†ìŒ")
            elif not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                briefing = generate_daily_briefing_pro_v3(target_date_str, daily_papers, openai_api_key, model_option)
                save_daily_column(target_date_str, briefing)
                st.success("ì™„ë£Œ!")
                st.rerun()
    with c2:
        st.subheader("ğŸ“¨ ê³µìœ  ë° ì „ì†¡")
        content = get_daily_column(target_date_str)
        if content:
            st.markdown("##### ğŸš€ í…”ë ˆê·¸ë¨ ì „ì†¡")
            user_footer = st.text_area("ğŸ“¢ ì¶”ê°€ ì½”ë©˜íŠ¸", height=70)
            final_msg = content
            if user_footer: final_msg += f"\n\n--------------------------------\nğŸ“¢ **Editor's Note**\n{user_footer}"

            if st.button("âœˆï¸ í…”ë ˆê·¸ë¨ ì „ì†¡", type="primary"):
                if not telegram_token or not chat_id: st.error("í† í° í•„ìš”")
                else:
                    try:
                        url = f"https://api.telegram.org/bot{telegram_token}/sendMessage"
                        res = requests.post(url, json={"chat_id": chat_id, "text": final_msg, "parse_mode": "Markdown"})
                        if res.status_code == 200: st.success("ì „ì†¡ ì™„ë£Œ")
                        else: st.error(f"ì‹¤íŒ¨: {res.text}")
                    except Exception as e: st.error(f"ì—ëŸ¬: {e}")
            st.divider()
            st.markdown(final_msg)
        else: st.warning("ë¸Œë¦¬í•‘ ì—†ìŒ")
# --- [Tab 2: ë¸”ë¡œê·¸ (ì°¸ê³ ë¬¸í—Œ ìë™ ì¶”ê°€ ê¸°ëŠ¥)] ---
# --- [Tab 2: ë¸”ë¡œê·¸ (ìƒìœ„ ê°œë… í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥ íƒ‘ì¬)] ---
with tab_blog:
    c_b1, c_b2 = st.columns([1, 3])
    with c_b1:
        st.subheader("âœ’ï¸ ì‹¬ì¸µ ë¸”ë¡œê·¸ ìƒì„±")
        b_date = st.date_input("ë‚ ì§œ", value=datetime.now(), key="blog_date")
        b_date_str = b_date.strftime("%Y-%m-%d")
        b_papers = get_papers_by_date(b_date_str)
        st.info(f"í›„ë³´: {len(b_papers)}ê±´")
        
        if not b_papers.empty:
            sel_title = st.selectbox("ë…¼ë¬¸ ì„ íƒ", b_papers['title_kr'].tolist())
            target_paper = b_papers[b_papers['title_kr'] == sel_title].iloc[0]
            uploaded_pdf = st.file_uploader("ğŸ“„ PDF ì—…ë¡œë“œ (ì„ íƒ)", type="pdf")
            b_model = st.selectbox("ëª¨ë¸:", ["gpt-4o", "gpt-4o-mini"], index=0)
            target_type = st.radio("íƒ€ê²Ÿ:", ["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš©", "ğŸ˜Š í™˜ììš©"])

            if st.button("ğŸš€ ì‹¬ì¸µ ë¶„ì„ & ê¸€ì“°ê¸°"):
                if not openai_api_key: st.error("Key ì—†ìŒ")
                else:
                    # 1. ë³¸ë¬¸ í…ìŠ¤íŠ¸ í™•ë³´
                    with st.spinner("1. ìë£Œ ë¶„ì„ ì¤‘... (PDF/PMC)"):
                        status_msg = "ì´ˆë¡(Abstract) ê¸°ë°˜"
                        content_source = target_paper['abstract']
                        if uploaded_pdf:
                            pdf_txt = read_pdf_file(uploaded_pdf)
                            if pdf_txt: content_source = pdf_txt; status_msg = "ğŸ“‚ PDF ì „ë¬¸ ë¶„ì„"
                        elif not uploaded_pdf:
                            pmc_txt, pmc_msg = fetch_pmc_fulltext(target_paper['pmid'])
                            if pmc_txt: content_source = pmc_txt; status_msg = pmc_msg

                    # 2. [í•µì‹¬ ë³€ê²½] ê²€ìƒ‰ì–´ 2ê°œ ìƒì„± (Specific / Broad)
                    with st.spinner("2. í™•ì¥í˜• êµì°¨ ê²€ì¦(Consensus) ì‹¤í–‰ ì¤‘..."):
                        client = OpenAI(api_key=openai_api_key)
                        
                        # AIì—ê²Œ "ì¢ì€ ê²€ìƒ‰ì–´"ì™€ "ë„“ì€ ê²€ìƒ‰ì–´"ë¥¼ ë™ì‹œì— ìš”ì²­
                        q_prompt = f"""
                        Analyze this text and generate TWO English search queries for PubMed validation.
                        
                        1. "Specific": Exact intervention + Exact disease (e.g., "Sopoongsan AND Atopic Dermatitis")
                        2. "Broad": Intervention class + Symptom/Disease category (e.g., "Herbal Medicine AND Pruritus", "TCM AND Eczema")
                        
                        * Only output valid JSON.
                        
                        Text: {content_source[:1500]}
                        
                        Output JSON format:
                        {{
                            "specific": "Query string 1",
                            "broad": "Query string 2"
                        }}
                        """
                        try:
                            q_resp = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user","content":q_prompt}]).choices[0].message.content
                            q_json = json.loads(re.search(r'\{.*\}', q_resp, re.DOTALL).group())
                            
                            q_specific = q_json['specific']
                            q_broad = q_json['broad']
                            
                            # 1ì°¨ ê²€ìƒ‰ (ì¢ì€ ë²”ìœ„)
                            evidence, ref_list = get_consensus_evidence(q_specific)
                            
                            # [ë¡œì§] ê²°ê³¼ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ -> ë„“ì€ ë²”ìœ„ ê²€ìƒ‰ ì¶”ê°€ ì‹¤í–‰!
                            if len(ref_list) < 3:
                                ev_broad, ref_broad = get_consensus_evidence(q_broad)
                                evidence += f"\n\n[ì¶”ê°€ ê·¼ê±° (ìƒìœ„/ìœ ì‚¬ ê³„ì—´)]: {q_broad}\n" + ev_broad
                                
                                # ì¤‘ë³µ ì œê±°í•˜ë©° ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
                                existing_urls = [r['url'] for r in ref_list]
                                for item in ref_broad:
                                    if item['url'] not in existing_urls:
                                        # ì¸ë±ìŠ¤ ë²ˆí˜¸ ì¡°ì •
                                        item['index'] = len(ref_list) + 1
                                        ref_list.append(item)
                                        
                        except Exception as e:
                            evidence = f"ê²€ì¦ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}"
                            ref_list = []

                    # 3. ê¸€ ì‘ì„± (ë¹„êµ/ë³´ì™„ ê´€ì  ì¶”ê°€)
                    with st.spinner("3. ì‹¬ì¸µ ì¹¼ëŸ¼ ì‘ì„± ì¤‘..."):
                        final_prompt = f"""
                        ë‹¹ì‹ ì€ ì„ìƒ í•œì˜í•™ ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤. 
                        ì œê³µëœ [ë…¼ë¬¸]ê³¼ [ê²€ì¦ìë£Œ]ë¥¼ ì¢…í•©í•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.

                        [ìƒí™©] {status_msg}
                        [ë©”ì¸ ë…¼ë¬¸] {content_source[:25000]}
                        
                        [ê²€ì¦ ìë£Œ (Consensus)] 
                        {evidence}

                        [ì‘ì„± í•µì‹¬ ì§€ì¹¨ - ë¹„êµì™€ ë³´ì™„]
                        [Role & Persona] ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ì„ìƒ í•œì˜ì‚¬ì´ì, ë‚œí•´í•œ ì˜í•™ ë…¼ë¬¸ì„ ì¼ë°˜ì¸ì˜ ì–¸ì–´ë¡œ ì‰½ê³  ì¬ì¹˜ ìˆê²Œ í’€ì–´ë‚´ëŠ” 'ê±´ê°• ì „ë¬¸ ì¹¼ëŸ¼ë‹ˆìŠ¤íŠ¸'ì…ë‹ˆë‹¤. ì‹ ë¢°ê° ìˆëŠ” ë§íˆ¬ë¥¼ ìœ ì§€í•˜ë˜, ë„¤ì´ë²„ ë¸”ë¡œê·¸ íŠ¹ìœ ì˜ ì¹œê·¼í•¨ê³¼ ê°€ë…ì„±ì„ ê°–ì¶˜ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.

[Context & Task] ì œê¸°ëœ ì£¼ì œì—ì„œì˜ í•™ìˆ ì  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, í•œì˜ì‚¬ê°€ ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.

[Specific Guidelines]

íƒ€ê²Ÿ ë…ì: ë©”ì¸ ì¤‘ì¬ì— ëŒ€í•œ ì§ˆí™˜ì´ ê°€ì¥ ì˜ ë°œìƒí•˜ëŠ” ì—°ë ¹ëŒ€ ë° ì„±ë³„ì„ ëŒ€ìƒìœ¼ë¡œ ë©”ì¸ ì¤‘ì¬ì˜ ëŒ€ìƒì§ˆí™˜ í™˜ì ë° ê·¸ë“¤ì˜ ìë…€ í˜¹ì€ ê°€ì¡± (ê°€ë…ì„±ì´ ë†’ê³  ë¬¸ì¥ì´ ëª…í™•í•´ì•¼ í•¨).

êµ¬ì¡°(Framework): 'ë¬¸ì œ ì œê¸° -> ìƒˆë¡œìš´ ëŒ€ì•ˆ ì œì‹œ(ì—°êµ¬ ê²°ê³¼) -> í•µì‹¬ ì•½ì¬ ì„¤ëª…(í•œì˜í•™ ì¤‘ì¬) -> ì „ë¬¸ê°€ì˜ ë¹„í‰ -> ê²°ë¡ ' ìˆœìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

ë°ì´í„° ë³´ì™„: > * ì œê³µëœ ë ˆí¼ëŸ°ìŠ¤ ì™¸ì— í•µì‹¬ ì¤‘ì¬ì˜ **Network Pharmacology(ë„¤íŠ¸ì›Œí¬ ì•½ë¦¬í•™)**ì  ê¸°ì „(ì˜ˆ: ì—¼ì¦ì„± ì‚¬ì´í† ì¹´ì¸ ì–µì œ ë“±) ë˜ëŠ” êµ­ê°€ì œê³µ ë°ì´í„°, ì œê³µë˜ì§€ ì•Šì•˜ì§€ë§Œ í™•ì‹¤í•˜ê²Œ í™•ì¸ëœ ì„ìƒë°ì´í„° í˜¹ì€ ë…¼ë¬¸ì„ ì¡°ì‚¬í•´ì„œ í•œê°œ ~ë‘ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì‰½ê²Œ ë§ë¶™ì—¬ì£¼ì„¸ìš”. ë§¤ë²ˆ ë„¤íŠ¸ì›Œí¬ ì•½ë¦¬í•™ë§Œ ê·¼ê±°ë¡œ ì ì„ ìˆ˜ëŠ” ì—†ìœ¼ë‹ˆ ë‹¤ë¥¸ ê·¼ê±°ë¥¼ ë„£ëŠ”ê²ƒë„ í•­ìƒ ê³ ë ¤í•´ì£¼ì„¸ìš”.

ë§Œì•½ì— ë…¼ë¬¸ì—ì„œ ì§€ì ëœ 'ë‚®ì€ ë°©ë²•ë¡ ì  ì§ˆ'ì„ ì–¸ê¸‰í•  ë•Œ, ë…ìê°€ ë¶ˆì•ˆí•´í•˜ì§€ ì•Šë„ë¡ ì•ìœ¼ë¡œ ë” ì •êµí•œ ì—°êµ¬ê°€ ê¸°ëŒ€ë˜ëŠ” ìœ ë§í•œ ë¶„ì•¼ë¼ëŠ” ì ì„ ê°•ì¡°í•˜ì„¸ìš”.

í†¤ì•¤ë§¤ë„ˆ: ì‹ ë¢°(80%) + ë‹¤ì •í•¨(20%). ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ê³ , ì „ë¬¸ ìš©ì–´ëŠ” ë°˜ë“œì‹œ ê´„í˜¸ë‚˜ ì‰¬ìš´ ë¹„ìœ ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.

SEO ìµœì í™”: ë©”ì¸ ì¹˜ë£Œë²•, ë©”ì¸ ì¹˜ë£Œë²•ì˜ ëŒ€ìƒ ì§ˆí™˜ ì¹˜ë£Œ, í•´ë‹¹ ì¤‘ì¬ì˜ íš¨ëŠ¥ ë“±ì˜ í‚¤ì›Œë“œê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì•„ë“¤ê²Œ í•˜ì„¸ìš”.
                        5. ë§ˆì§€ë§‰ì—ëŠ” [Reference] ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¬ì§€ ë§ˆì„¸ìš”. (ì‹œìŠ¤í…œì´ ì•Œì•„ì„œ ë‹µë‹ˆë‹¤.)
                        """
                        article = client.chat.completions.create(model=b_model, messages=[{"role":"user","content":final_prompt}]).choices[0].message.content
                        
                        # ì°¸ê³ ë¬¸í—Œ ë¶€ì°©
                        if ref_list:
                            article += "\n\n---\n### ğŸ“š ì°¸ê³  ë¬¸í—Œ (References)\n"
                            article += f"**[Main]** {target_paper['title_kr']} (PMID: {target_paper['pmid']})\n"
                            for ref in ref_list:
                                article += f"{ref['index']}. [{ref['title']}]({ref['url']})\n"

                        save_blog_post(b_date_str, "doctor" if "ì „ë¬¸ê°€" in target_type else "patient", article)
                        st.success(f"ì‘ì„± ì™„ë£Œ! (ì°¸ê³ ë¬¸í—Œ {len(ref_list)}ê±´ í™•ë³´)")
                        st.rerun()

    with c_b2:
        st.subheader("ğŸ“„ ë¯¸ë¦¬ë³´ê¸°")
        t1, t2 = st.tabs(["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš©", "ğŸ˜Š í™˜ììš©"])
        with t1:
            post = get_blog_post(b_date_str, "doctor")
            if post: st.markdown(post)
        with t2:
            post = get_blog_post(b_date_str, "patient")
            if post: st.markdown(post)

# --- [Tab 3: ë³´ê´€í•¨] ---
with tab_archive:
    df_all = pd.read_sql("SELECT * FROM papers", sqlite3.connect(DB_NAME))
    if not df_all.empty:
        st.subheader("ğŸ” í•„í„°ë§")
        cats = sorted(df_all['intervention_category'].unique().tolist())
        sel_cats = st.multiselect("ì¤‘ì¬ë²•", cats, default=cats)
        df_filt = df_all[df_all['intervention_category'].isin(sel_cats)] if sel_cats else df_all.copy()
        
        if not df_filt.empty:
            df_filt.insert(0, "del", False)
            df_filt["url"] = "https://pubmed.ncbi.nlm.nih.gov/" + df_filt["pmid"]
            
            edited = st.data_editor(
                df_filt,
                column_config={
                    "del": st.column_config.CheckboxColumn("ì‚­ì œ", width="small"),
                    "url": st.column_config.LinkColumn("Link"),
                    "tags": st.column_config.TextColumn("íƒœê·¸"),
                    "n_count": st.column_config.TextColumn("Nìˆ˜"),
                    "user_note": st.column_config.TextColumn("ë©”ëª¨")
                },
                column_order=["del", "url", "clinical_score", "tags", "n_count", "title_kr", "user_note"],
                hide_index=True, use_container_width=True
            )
            
            if st.button("ğŸ—‘ï¸ ì‚­ì œ í™•ì¸"):
                to_del = edited[edited["del"]]['pmid'].tolist()
                if to_del: delete_papers(to_del); st.rerun()

            st.divider()
            st.caption("ğŸ‘‡ ë…¼ë¬¸ ìƒì„¸ ê²€ì¦")
            for _, row in df_filt.iterrows():
                with st.expander(f"{row['title_kr']}"):
                    st.info(row['summary'])
                    c1, c2, c3 = st.columns(3)
                    c1.link_button("ğŸ“„ ì›ë¬¸", row['url'], use_container_width=True)
                    q = urllib.parse.quote(row['original_title'])
                    c2.link_button("âš–ï¸ Consensus", f"https://consensus.app/results/?q={q}", use_container_width=True)
                    c3.link_button("ğŸ¤– SciSpace", f"https://typeset.io/search?q={q}", use_container_width=True)

# --- [Tab 4: ê²€ìƒ‰] ---
with tab_search:
    c1, c2 = st.columns(2)
    s_d = c1.date_input("ì‹œì‘", datetime.now()-timedelta(days=2))
    e_d = c2.date_input("ë", datetime.now())
    limit = st.slider("ê°œìˆ˜", 10, 100, 50)
    
    if 'search_res' not in st.session_state: st.session_state.search_res = None
    if st.button("1. ê²€ìƒ‰"):
        with st.spinner(".."): st.session_state.search_res = search_pubmed_raw(s_d, e_d, limit)
        
    if st.session_state.search_res:
        df = pd.DataFrame(st.session_state.search_res)
        df.insert(0, "Sel", ~df['is_saved'])
        edited = st.data_editor(df, column_config={"Sel": st.column_config.CheckboxColumn("ì„ íƒ")}, hide_index=True)
        targets = edited[edited["Sel"]]
        
        if st.button(f"2. {len(targets)}ê±´ ë¶„ì„ ë° ì €ì¥"):
            if not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                conn = sqlite3.connect(DB_NAME); cur = conn.cursor()
                bar = st.progress(0)
                
                # í…Œì´ë¸” ìƒì„±
                cur.execute('''CREATE TABLE IF NOT EXISTS papers (
                    pmid TEXT PRIMARY KEY, date_published TEXT, title_kr TEXT, 
                    intervention_category TEXT, target_body_part TEXT, specific_point TEXT, 
                    study_design TEXT, clinical_score INTEGER, summary TEXT, 
                    original_title TEXT, abstract TEXT, icd_code TEXT, full_text_status TEXT,
                    n_count TEXT, p_value TEXT, tags TEXT, user_note TEXT
                )''')
                
                full_list = [p for p in st.session_state.search_res if p['pmid'] in targets['pmid'].tolist()]
                for i, p in enumerate(full_list):
                    bar.progress((i+1)/len(full_list))
                    res = analyze_paper_strict(p, openai_api_key)
                    meta = analyze_metadata_free(p['abstract'], p['title'])
                    
                    if "error" not in res:
                        cur.execute("INSERT OR REPLACE INTO papers VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)", (
                            p['pmid'], datetime.now().strftime('%Y-%m-%d'),
                            res.get('korean_title'), res.get('intervention_category'),
                            res.get('target_body_part'), res.get('specific_point'),
                            res.get('study_design'), res.get('clinical_score'),
                            res.get('summary'), p['title'], p['abstract'], 
                            res.get('icd_code'), "Abstract Saved",
                            meta['n_count'], meta['p_value'], meta['tags_str'], ""
                        ))
                        conn.commit()
                conn.close(); db.push_db()
                st.success("ì™„ë£Œ!"); st.session_state.search_res = None; st.rerun()

if __name__ == "__main__":
    if not st.session_state.get('db_synced'):
        db.pull_db()
        st.session_state.db_synced = True
    migrate_db()










