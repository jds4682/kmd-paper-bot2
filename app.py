import streamlit as st
import sqlite3
import pandas as pd
import json
import re
from Bio import Entrez
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from openai import OpenAI
import time

# ===================== [ì„¤ì • ë° ì´ˆê¸°í™”] =====================
st.set_page_config(page_title="í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro", layout="wide", page_icon="ğŸ¥")

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    email_address = st.text_input("Email (PubMedìš©)", value="your_email@example.com")
    st.info("ğŸ’¡ Top ë…¼ë¬¸ì€ ì›ë¬¸(Full Text)ì„ í™•ë³´í•˜ì—¬ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.")

Entrez.email = email_address
DB_NAME = 'kmd_papers_v5_column.db' 

# ===================== [1. DB ê´€ë¦¬ (ìë™ ì—…ë°ì´íŠ¸ ê¸°ëŠ¥ ì¶”ê°€)] =====================
def init_db():
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    
    # 1. ë…¼ë¬¸ í…Œì´ë¸” ìƒì„± (ê¸°ë³¸)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS papers (
            pmid TEXT PRIMARY KEY,
            date_published TEXT,
            title_kr TEXT,
            intervention_category TEXT, 
            target_body_part TEXT,      
            specific_point TEXT,        
            study_design TEXT,
            clinical_score INTEGER,
            summary TEXT,
            original_title TEXT,
            abstract TEXT,
            icd_code TEXT,
            full_text_status TEXT
        )
    ''')
    
    # ğŸš¨ DB ë§ˆì´ê·¸ë ˆì´ì…˜ (ì—ëŸ¬ í•´ê²° í•µì‹¬ ë¡œì§)
    # ê¸°ì¡´ DBì— 'full_text_status' ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€í•´ì£¼ëŠ” ì½”ë“œ
    cursor.execute("PRAGMA table_info(papers)")
    columns = [info[1] for info in cursor.fetchall()]
    if 'full_text_status' not in columns:
        try:
            cursor.execute("ALTER TABLE papers ADD COLUMN full_text_status TEXT")
            st.toast("ì‹œìŠ¤í…œ: DB ì—…ë°ì´íŠ¸ ì™„ë£Œ (full_text_status ì»¬ëŸ¼ ì¶”ê°€ë¨)")
        except:
            pass # ì´ë¯¸ ìˆìœ¼ë©´ íŒ¨ìŠ¤

    # 2. ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS daily_columns (
            date_id TEXT PRIMARY KEY,
            content TEXT,
            created_at TEXT
        )
    ''')
    
    # 3. ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ í…Œì´ë¸” (êµ¬ë²„ì „ ì¶©ëŒ ë°©ì§€)
    try:
        cursor.execute("SELECT target_type FROM blog_posts LIMIT 1")
    except sqlite3.OperationalError:
        cursor.execute("DROP TABLE IF EXISTS blog_posts")
        conn.commit()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS blog_posts (
            date_id TEXT,
            target_type TEXT, 
            content TEXT,
            created_at TEXT,
            PRIMARY KEY (date_id, target_type)
        )
    ''')
    
    # 4. [NEW] ì‹œìŠ¤í…œ ì„¤ì • í…Œì´ë¸” (ìë™í™” ì œì–´ìš©)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS system_config (
            key TEXT PRIMARY KEY,
            value TEXT
        )
    ''')
    
    conn.commit()
    conn.close()

# ì„¤ì • ì €ì¥/ë¡œë“œ í•¨ìˆ˜
def set_config(key, value):
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()
    cur.execute("INSERT OR REPLACE INTO system_config VALUES (?, ?)", (key, str(value)))
    conn.commit()
    conn.close()

def get_config(key):
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()
    cur.execute("SELECT value FROM system_config WHERE key=?", (key,))
    res = cur.fetchone()
    conn.close()
    return res[0] if res else "False"

def get_papers_by_date(target_date_str):
    conn = sqlite3.connect(DB_NAME)
    try:
        query = "SELECT * FROM papers WHERE date_published = ?"
        df = pd.read_sql(query, conn, params=(target_date_str,))
    except:
        df = pd.DataFrame()
    conn.close()
    return df

# ë¸Œë¦¬í•‘ ì €ì¥/ë¡œë“œ
def get_daily_column(date_str):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("SELECT content FROM daily_columns WHERE date_id = ?", (date_str,))
    result = cursor.fetchone()
    conn.close()
    return result[0] if result else None

def save_daily_column(date_str, content):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO daily_columns VALUES (?, ?, ?)", 
                   (date_str, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit()
    conn.close()

# ë¸”ë¡œê·¸ ì €ì¥/ë¡œë“œ
def get_blog_post(date_str, target_type):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT content FROM blog_posts WHERE date_id = ? AND target_type = ?", (date_str, target_type))
        result = cursor.fetchone()
        return result[0] if result else None
    except sqlite3.OperationalError:
        return None

def save_blog_post(date_str, target_type, content):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO blog_posts VALUES (?, ?, ?, ?)", 
                   (date_str, target_type, content, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    conn.commit()
    conn.close()

def delete_papers(pmid_list):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    if pmid_list:
        placeholders = ', '.join('?' for _ in pmid_list)
        cursor.execute(f"DELETE FROM papers WHERE pmid IN ({placeholders})", pmid_list)
        conn.commit()
    conn.close()

def check_if_exists(pmid):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM papers WHERE pmid=?", (pmid,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ===================== [2. Full Text Fetcher] =====================
def fetch_pmc_fulltext(pmid):
    try:
        link_results = Entrez.elink(dbfrom="pubmed", db="pmc", id=pmid)
        if not link_results or not link_results[0]['LinkSetDb']:
            return None, "Abstract Only"
        
        pmc_id = link_results[0]['LinkSetDb'][0]['Link'][0]['Id']
        handle = Entrez.efetch(db="pmc", id=pmc_id, rettype="xml")
        xml_data = handle.read()
        handle.close()
        
        root = ET.fromstring(xml_data)
        full_text = ""
        for body in root.findall(".//body"):
            for text in body.itertext():
                full_text += text + " "
        
        if len(full_text) > 500:
            return full_text[:25000], "âœ… Full Text (PMC)" 
        else:
            return None, "PMC XML Empty"
    except Exception as e:
        return None, f"Error: {str(e)}"

# ===================== [3. AI ë¶„ì„ ë¡œì§] =====================
def analyze_paper_strict(paper_data, api_key):
    client = OpenAI(api_key=api_key)
    prompt = f"""
    ë„ˆëŠ” ì„ìƒ í•œì˜í•™ ë…¼ë¬¸ ì‹¬ì‚¬ê´€ì´ë‹¤.
    
    [í•„ìˆ˜ ê·œì¹™]
    0. **ë™ë¬¼/ì„¸í¬ ì‹¤í—˜ì€ ë¬´ì¡°ê±´ DROP.**
    1. 'clinical_score': 1~10ì  (ê·¼ê³¨ê²©/ì†Œí™”ê¸°/í†µì¦ ë“± ë¡œì»¬ ë‹¤ë¹ˆë„ ì§ˆí™˜ ê°€ì‚°ì ).
    2. 'specific_point': ì²˜ë°©ëª…(êµ¬ì„±/gìˆ˜), í˜ˆìë¦¬ í•„ìˆ˜.
    3. 'study_design': RCT, SR, Case Report, Cohort.

    [JSON í˜•ì‹]
    {{
        "korean_title": "í•œê¸€ ì œëª©",
        "study_design": "ì—°êµ¬ ìœ í˜•",
        "intervention_category": "ì¹´í…Œê³ ë¦¬",
        "specific_point": "ìƒì„¸ ì¤‘ì¬ ë‚´ìš©",
        "target_body_part": "ì‹ ì²´ë¶€ìœ„",
        "clinical_score": 8,
        "summary": "3ì¤„ ìš”ì•½",
        "icd_code": "ì½”ë“œ",
        "full_text_status": "Abstract Check"
    }}

    Title: {paper_data['title']}
    Abstract: {paper_data['abstract']}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        data = json.loads(re.search(r'\{.*\}', response.choices[0].message.content.strip(), re.DOTALL).group())
        if "DROP" in str(data.get("study_design", "")): return {"error": "DROP: ì„ìƒ ì—°êµ¬ ì•„ë‹˜"}
        return data
    except Exception as e:
        return {"error": str(e)}

# ===================== [4. PubMed ê²€ìƒ‰] =====================
def simple_keyword_classify(text):
    text = text.lower()
    if "acupuncture" in text or "needling" in text: return "1_ì¹¨êµ¬ì¹˜ë£Œ"
    elif "herbal" in text or "decoction" in text: return "2_í•œì•½ì¹˜ë£Œ"
    elif "chuna" in text or "manipulation" in text: return "5_ì¶”ë‚˜/ë„ìˆ˜"
    else: return "7_ê¸°íƒ€/ë³µí•©"

def search_pubmed_raw(start_date, end_date, max_results):
    str_start = start_date.strftime("%Y/%m/%d")
    str_end = end_date.strftime("%Y/%m/%d")
    search_term = """
    ("TCM" OR "Traditional chinese medicine" OR "Herbal medicine" OR "Medicine, Korean Traditional" 
    OR "Acupuncture" OR "Moxibustion" OR "Cupping Therapy" OR "Pharmacopuncture" OR "Chuna") 
    AND (hasabstract[text]) AND ("Humans"[Mesh]) 
    AND ("Case Reports"[ptyp] OR "Clinical Trial"[ptyp] OR "Randomized Controlled Trial"[ptyp] OR "Systematic Review"[ptyp] OR "Cohort Studies"[Mesh])
    """
    try:
        handle = Entrez.esearch(db="pubmed", term=search_term, mindate=str_start, maxdate=str_end, datetype="pdat", retmax=max_results)
        record = Entrez.read(handle)
        handle.close()
        id_list = record["IdList"]
    except: return []

    if not id_list: return []

    try:
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="xml")
        records = Entrez.read(handle)
        handle.close()
    except: return []

    raw_papers = []
    for article in records['PubmedArticle']:
        try:
            pmid = str(article['MedlineCitation']['PMID'])
            title = article['MedlineCitation']['Article']['ArticleTitle']
            abstract_list = article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])
            abstract = " ".join(abstract_list) if abstract_list else ""
            
            raw_papers.append({
                "pmid": pmid,
                "title": title,
                "abstract": abstract,
                "predicted_category": simple_keyword_classify(title + abstract),
                "is_saved": check_if_exists(pmid)
            })
        except: continue
    return raw_papers

# ===================== [5. ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ìƒì„±ê¸°] =====================
def generate_daily_briefing_pro_v3(date_str, papers_df, api_key, model_choice):
    client = OpenAI(api_key=api_key)
    top_papers = papers_df.sort_values(by='clinical_score', ascending=False).head(10)
    
    if top_papers.empty: return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    analyzed_data = []
    
    prog_bar = st.progress(0)
    status_text = st.empty()

    for idx, (_, row) in enumerate(top_papers.iterrows()):
        prog_bar.progress((idx+1)/len(top_papers))
        status_text.text(f"ğŸ” ì•ˆì „ ë¶„ì„ ëª¨ë“œ ë™ì‘ ì¤‘... ({idx+1}): {row['title_kr']}")
        
        full_text, ft_status = fetch_pmc_fulltext(row['pmid'])
        content_source = full_text if full_text else row['abstract']
        
        pico_prompt = f"""
        ì´ ë…¼ë¬¸ì„ PICO êµ¬ì¡°ë¡œ ë¶„ì„í•˜ë¼.

        [ğŸš¨ ì•ˆì „ ë¶„ì„ ê·œì¹™]
        1. **ì•½ì–´(Acronym):** í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì •ì˜ëœ Full Nameì„ ì°¾ì•„ë¼. ì •ì˜ê°€ ì—†ìœ¼ë©´ 'Unknown'ìœ¼ë¡œ í‘œê¸°.
        2. **ìƒì„¸ ì •ë³´:** ì•½ì¬ ìš©ëŸ‰(g), íšŸìˆ˜ ë“±ì´ ì—†ìœ¼ë©´ 'ë³¸ë¬¸ ë¯¸ê¸°ì¬' í‘œê¸°.
        
        [ë¶„ì„ í•­ëª©]
        - valid: true/false
        - P: í™˜ì ì •ë³´
        - I: ì¤‘ì¬ (ì•½ì–´ëŠ” Full name ë³€í™˜, ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ')
        - C: ëŒ€ì¡°êµ°
        - O: ê²°ê³¼ (p-value í¬í•¨, ì—†ìœ¼ë©´ 'ìˆ˜ì¹˜ ë¯¸ê¸°ì¬')

        Title: {row['title_kr']}
        Text: {content_source[:15000]}
        """
        
        try:
            pico_res_text = client.chat.completions.create(
                model="gpt-4o-mini", 
                messages=[{"role": "user", "content": pico_prompt}],
                temperature=0.0
            ).choices[0].message.content
        except: pico_res_text = "ë¶„ì„ ì‹¤íŒ¨"

        analyzed_data.append({
            "pmid": row['pmid'],
            "title": row['title_kr'],
            "score": row['clinical_score'],
            "study_design": row['study_design'],
            "source": ft_status,
            "detail_analysis": pico_res_text
        })

    status_text.empty()
    prog_bar.empty()

    final_prompt = f"""
    ë‹¹ì‹ ì€ íŒ©íŠ¸ ì²´í¬ë¥¼ ì¤‘ìš”ì‹œí•˜ëŠ” í•œì˜í•™ ì—ë””í„°ì…ë‹ˆë‹¤.
    ì œê³µëœ {len(analyzed_data)}ê°œì˜ ë¶„ì„ ë°ì´í„°ë¥¼ ê²€í† í•˜ì—¬ **ìƒìœ„ 7ê°œ(Pick 2 + News 5)**ë§Œ ì„ ë³„í•˜ì—¬ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

    [ì„ ë³„ ë° ì‘ì„± ê·œì¹™]
    1. **ì—„ê²©í•œ í•„í„°ë§:** ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì•½ì–´ê°€ ë¶ˆë¶„ëª…í•œ ë…¼ë¬¸ì€ ì œì™¸í•˜ì„¸ìš”.
    2. **ë‚´ìš© ì¶©ì‹¤:** News ì„¹ì…˜ë„ ìƒì„¸í•˜ê²Œ ì‘ì„±.
    3. **í•„ìˆ˜ ë§í¬:** ê° ë…¼ë¬¸ì˜ ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ ì›ë¬¸ ë§í¬ë¥¼ ë‹¬ì•„ì£¼ì„¸ìš”.
       - í˜•ì‹: `ğŸ”— ì›ë¬¸: https://pubmed.ncbi.nlm.nih.gov/[PMID]`
       - ì…ë ¥ ë°ì´í„°ì˜ 'pmid' ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    
    [ì¶œë ¥ í¬ë§·]
    ğŸ“… **{date_str} í•œì˜ ì„ìƒ ë¸Œë¦¬í•‘**
    
    ğŸ¥‡ **Today's Pick 1: [ì œëª©]**
    ([ì—°êµ¬ìœ í˜•] / â­[ì ìˆ˜])
    - ğŸ¯ **Point:** ...
    - ğŸ’Š **Method:** ...
    - ğŸ“Š **Result:** ...
    - ğŸ” **Check:** (ì›ë¬¸ ë¶„ì„ ì—¬ë¶€)
    - ğŸ”— **ì›ë¬¸:** https://pubmed.ncbi.nlm.nih.gov/[PMID]

    ğŸ¥ˆ **Today's Pick 2: [ì œëª©]**
    (ë™ì¼ ì–‘ì‹)

    --------------------------------
    
    ğŸ“° **Clinical News Top 5 (ìƒì„¸)**
    
    1ï¸âƒ£ **[ì œëª©]**
       - ğŸ“ **ì¹˜ë£Œ:** ...
       - ğŸ“‰ **ê²°ê³¼:** ...
       - ğŸ”— **ì›ë¬¸:** https://pubmed.ncbi.nlm.nih.gov/[PMID]
    
    [ì…ë ¥ ë°ì´í„°]
    {json.dumps(analyzed_data, ensure_ascii=False)}
    """
    
    try:
        if "o1" in model_choice:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": final_prompt}]).choices[0].message.content
        else:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": final_prompt}], temperature=0.7).choices[0].message.content
    except Exception as e: return f"ìƒì„± ì‹¤íŒ¨: {e}"

# ===================== [6. ë¸”ë¡œê·¸ ì•„í‹°í´ ìƒì„±ê¸°] =====================
def generate_blog_article(date_str, papers_df, api_key, model_choice, target_audience="doctor"):
    client = OpenAI(api_key=api_key)
    
    top_paper = papers_df.sort_values(by='clinical_score', ascending=False).iloc[0]
    full_text, ft_status = fetch_pmc_fulltext(top_paper['pmid'])
    content_source = full_text if full_text else top_paper['abstract']
    
    if target_audience == "doctor":
        prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ ì˜í•™ ë¸”ë¡œê±°ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì˜ Top ë…¼ë¬¸ 1í¸ì„ ì„ ì •í•˜ì—¬ **1500ì ë‚´ì™¸ì˜ ì‹¬ì¸µ ì „ë¬¸ ì•„í‹°í´**ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ëª©í‘œ] 'êµ¬ê¸€ ì• ë“œì„¼ìŠ¤' ìˆ˜ìµí˜• ë¸”ë¡œê·¸ì— ì í•©í•œ ê³ í’ˆì§ˆ ì½˜í…ì¸ .

        [ë…¼ë¬¸ ë°ì´í„°]
        ì œëª©: {top_paper['title_kr']} ({top_paper['original_title']})
        PMID: {top_paper['pmid']}
        ë‚´ìš©: {content_source[:20000]}
        """
    else:
        prompt = f"""
        ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ì‹¤ë ¥ ìˆëŠ” ë™ë„¤ í•œì˜ì› ì›ì¥ë‹˜ì…ë‹ˆë‹¤. 
        ì´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¼ë°˜ì¸ í™˜ìë“¤ì´ ì½ê¸° ì‰¬ìš´ **ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…**ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ëª©í‘œ] í™˜ìë“¤ì—ê²Œ ì‹ ë¢°ë¥¼ ì£¼ê³  ë‚´ì› ìœ ë„.

        [ë…¼ë¬¸ ë°ì´í„°]
        ì œëª©: {top_paper['title_kr']}
        PMID: {top_paper['pmid']}
        ë‚´ìš©: {content_source[:20000]}
        """
    
    try:
        if "o1" in model_choice:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
        else:
            return client.chat.completions.create(model=model_choice, messages=[{"role": "user", "content": prompt}], temperature=0.7).choices[0].message.content
    except Exception as e: return f"ë¸”ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}"

# ===================== [7. UI êµ¬ì„±] =====================
init_db()

st.title("ğŸ¥ í•œì˜í•™ ë…¼ë¬¸ AI íë ˆì´í„° Pro")
st.markdown("---")

tab_briefing, tab_blog, tab_archive, tab_search = st.tabs(["ğŸ“ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘", "âœï¸ ë¸”ë¡œê·¸/ìˆ˜ìµí™”", "ğŸ“š ë³´ê´€í•¨", "ğŸ” ê²€ìƒ‰"])

# --- [Tab 1: ë°ì¼ë¦¬ ë¸Œë¦¬í•‘] ---
with tab_briefing:
    c1, c2 = st.columns([1, 2])
    with c1:
        st.subheader("ğŸ—“ ë¸Œë¦¬í•‘ ì„¤ì •")
        target_date = st.date_input("ë‚ ì§œ", value=datetime.now())
        target_date_str = target_date.strftime("%Y-%m-%d")
        daily_papers = get_papers_by_date(target_date_str)
        st.info(f"ë…¼ë¬¸ ìˆ˜: {len(daily_papers)}ê±´")
        
        model_option = st.radio("AI ëª¨ë¸:", ["gpt-4o", "o1-preview", "gpt-4o-mini"], index=0)
        
        if st.button("âœ¨ ì•ˆì „ ëª¨ë“œ ë¸Œë¦¬í•‘ ìƒì„±"):
            if daily_papers.empty: st.error("ë…¼ë¬¸ ì—†ìŒ")
            elif not openai_api_key: st.error("Key ì—†ìŒ")
            else:
                briefing = generate_daily_briefing_pro_v3(target_date_str, daily_papers, openai_api_key, model_option)
                save_daily_column(target_date_str, briefing)
                st.rerun()

    with c2:
        st.subheader("ğŸ“¨ ê³µìœ  ë° ì»¤ìŠ¤í…€")
        content = get_daily_column(target_date_str)
        
        if content:
            st.markdown("##### ğŸ“¢ ì¶”ê°€ ì½”ë©˜íŠ¸")
            user_footer = st.text_area("í•˜ë‹¨ ì•ˆë‚´ë¬¸êµ¬", height=100)
            
            final_display_text = content
            if user_footer:
                final_display_text += "\n\n--------------------------------\nğŸ“¢ **Editor's Note**\n" + user_footer
            
            st.code(final_display_text, language='markdown')
            st.caption("ğŸ‘† ìš°ì¸¡ ìƒë‹¨ ì•„ì´ì½˜ í´ë¦­ ì‹œ ì „ì²´ ë³µì‚¬")
            
            with st.expander("ë¯¸ë¦¬ë³´ê¸°"):
                st.markdown(final_display_text)
        else:
            st.warning("ìƒì„±ëœ ë¸Œë¦¬í•‘ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- [Tab 2: ë¸”ë¡œê·¸/ìˆ˜ìµí™”] ---
with tab_blog:
    c_b1, c_b2 = st.columns([1, 3])
    with c_b1:
        st.subheader("âœ’ï¸ ë¸”ë¡œê·¸ ìƒì„±")
        b_date = st.date_input("ë¸”ë¡œê·¸ ë‚ ì§œ", value=datetime.now(), key="blog_date")
        b_date_str = b_date.strftime("%Y-%m-%d")
        b_papers = get_papers_by_date(b_date_str)
        st.info(f"í›„ë³´ ë…¼ë¬¸: {len(b_papers)}ê±´")
        
        b_model = st.selectbox("ì‘ì„± ëª¨ë¸:", ["gpt-4o", "o1-preview"], index=0)
        
        st.divider()
        target_type = st.radio(
            "íƒ€ê²Ÿ ë…ì:", 
            ["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš© (í‹°ìŠ¤í† ë¦¬)", "ğŸ˜Š í™˜ììš© (ë„¤ì´ë²„)"]
        )
        target_code = "doctor" if "ì „ë¬¸ê°€" in target_type else "patient"

        if st.button("âœï¸ ê¸€ ìë™ ìƒì„±"):
            if b_papers.empty: st.error("ë…¼ë¬¸ ì—†ìŒ")
            elif not openai_api_key: st.error("API Key í•„ìš”")
            else:
                with st.spinner("ì‘ì„± ì¤‘..."):
                    article = generate_blog_article(b_date_str, b_papers, openai_api_key, b_model, target_code)
                    save_blog_post(b_date_str, target_code, article)
                    st.success("ì™„ë£Œ!")
                    st.rerun()

    with c_b2:
        st.subheader("ğŸ“„ ë¯¸ë¦¬ë³´ê¸°")
        tab_doc, tab_pat = st.tabs(["ğŸ‘¨â€âš•ï¸ ì „ë¬¸ê°€ìš©", "ğŸ˜Š í™˜ììš©"])
        with tab_doc:
            doc_post = get_blog_post(b_date_str, "doctor")
            if doc_post:
                st.markdown(doc_post)
                st.divider()
                st.code(doc_post, language='markdown')
            else: st.info("ì—†ìŒ")
        with tab_pat:
            pat_post = get_blog_post(b_date_str, "patient")
            if pat_post:
                st.markdown(pat_post)
                st.divider()
                st.code(pat_post, language='markdown')
            else: st.info("ì—†ìŒ")

# --- [Tab 3: ë³´ê´€í•¨] ---
with tab_archive:
    df_all = pd.read_sql("SELECT * FROM papers", sqlite3.connect(DB_NAME))

    if df_all.empty:
        st.info("ë³´ê´€í•¨ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ íƒ­ì—ì„œ ë…¼ë¬¸ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    else:
        st.subheader("ğŸ” ë³´ê´€í•¨ í•„í„°ë§")
        
        all_categories = sorted(df_all['intervention_category'].unique().tolist())
        selected_categories = st.multiselect(
            "ì¤‘ì¬ë²• ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", 
            all_categories, 
            default=all_categories
        )

        if 'archive_body_part' not in st.session_state:
            st.session_state.archive_body_part = "ì „ì²´"

        def get_archive_btn_color(part):
            return "primary" if st.session_state.archive_body_part == part else "secondary"

        st.markdown("##### ì‹ ì²´ ë¶€ìœ„ë³„ ë³´ê¸°")
        col_b1, col_b2, col_b3, col_b4, col_b5, col_b6 = st.columns(6)
        
        parts_map = {
            "ë‘ê²½ë¶€": "ğŸ§ ", "ì²™ì¶”/í—ˆë¦¬": "ğŸ¦´", "ìƒì§€": "ğŸ’ª", 
            "í•˜ì§€": "ğŸ¦µ", "ë‚´ì¥ê¸°/ì „ì‹ ": "ğŸ«€", "ì „ì²´": "ğŸ”"
        }
        
        for i, (part, emoji) in enumerate(parts_map.items()):
            col = [col_b1, col_b2, col_b3, col_b4, col_b5, col_b6][i]
            with col:
                if st.button(f"{emoji} {part}", key=f"arc_btn_{i}", type=get_archive_btn_color(part), use_container_width=True):
                    st.session_state.archive_body_part = part
                    st.rerun()

        df_filtered = df_all.copy()
        
        if selected_categories:
            df_filtered = df_filtered[df_filtered['intervention_category'].isin(selected_categories)]
        else:
            df_filtered = pd.DataFrame() 

        if st.session_state.archive_body_part != "ì „ì²´":
            df_filtered = df_filtered[df_filtered['target_body_part'] == st.session_state.archive_body_part]

        st.divider()

        st.subheader(f"ğŸ“š ë…¼ë¬¸ ëª©ë¡ ({len(df_filtered)}ê±´)")
        
        if not df_filtered.empty:
            df_filtered.insert(0, "delete_sel", False)
            df_filtered["url"] = "https://pubmed.ncbi.nlm.nih.gov/" + df_filtered["pmid"]
            
            edited_df = st.data_editor(
                df_filtered,
                column_config={
                    "delete_sel": st.column_config.CheckboxColumn("ì‚­ì œ", width="small"),
                    "url": st.column_config.LinkColumn("Link", display_text="ğŸ”—", width="small"),
                    "title_kr": st.column_config.TextColumn("ì œëª©", width="large"),
                    "summary": st.column_config.TextColumn("ìš”ì•½", width="large"),
                    "date_published": st.column_config.TextColumn("ìˆ˜ì§‘ì¼", width="medium"),
                    "clinical_score": st.column_config.NumberColumn("ì ìˆ˜", format="%dì ", width="small"),
                    "full_text_status": st.column_config.TextColumn("ë¶„ì„ì¶œì²˜", width="medium"),
                },
                column_order=["delete_sel", "url", "clinical_score", "title_kr", "intervention_category", "summary", "full_text_status", "date_published"],
                hide_index=True,
                use_container_width=True,
                key="archive_editor"
            )

            to_delete = edited_df[edited_df["delete_sel"] == True]
            if not to_delete.empty:
                st.warning(f"{len(to_delete)}ê±´ì˜ ë…¼ë¬¸ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")
                if st.button("ğŸ—‘ï¸ ì„ íƒí•œ ë…¼ë¬¸ ì˜êµ¬ ì‚­ì œ", type="primary"):
                    delete_papers(to_delete['pmid'].tolist())
                    st.success("ì‚­ì œ ì™„ë£Œ!")
                    st.rerun()
        else:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- [Tab 4: ê²€ìƒ‰] ---
with tab_search:
    col1, col2 = st.columns(2)
    with col1: s_date = st.date_input("ì‹œì‘", value=datetime.now()-timedelta(days=2))
    with col2: e_date = st.date_input("ì¢…ë£Œ", value=datetime.now())
    
    # [ìˆ˜ì •] max_results ìŠ¬ë¼ì´ë” ë³µêµ¬ ë° ëª…í™•í•œ ìœ„ì¹˜ ë°°ì¹˜
    max_results = st.select_slider("ê²€ìƒ‰í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜", options=[10, 30, 50, 100, 300], value=50)

    if 'search_res' not in st.session_state: st.session_state.search_res = None

    if st.button("1. PubMed ê²€ìƒ‰ (ë¬´ë£Œ)"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            st.session_state.search_res = search_pubmed_raw(s_date, e_date, max_results)
    
    if st.session_state.search_res:
        df_res = pd.DataFrame(st.session_state.search_res)
        df_res.insert(0, "Sel", ~df_res['is_saved'])
        
        st.subheader(f"ê²€ìƒ‰ ê²°ê³¼: {len(df_res)}ê±´")
        edited_res = st.data_editor(df_res, column_config={"Sel": st.column_config.CheckboxColumn("ì„ íƒ")}, hide_index=True)
        targets = edited_res[edited_res["Sel"]]
        
        # [ìˆ˜ì •] ë²„íŠ¼ ì´ë¦„ ë³€ê²½
        if st.button(f"ğŸš€ 2. ì„ íƒí•œ {len(targets)}ê±´ AI ë¶„ì„ ë° ì €ì¥"):
            if not openai_api_key: st.error("Key Missing")
            else:
                conn = sqlite3.connect(DB_NAME)
                cur = conn.cursor()
                bar = st.progress(0)
                target_pmids = targets['pmid'].tolist()
                full_list = [p for p in st.session_state.search_res if p['pmid'] in target_pmids]
                for i, p in enumerate(full_list):
                    bar.progress((i+1)/len(full_list))
                    res = analyze_paper_strict(p, openai_api_key) 
                    if "error" not in res:
                        cur.execute('INSERT OR REPLACE INTO papers VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', (
                            p['pmid'], datetime.now().strftime('%Y-%m-%d'),
                            res.get('korean_title'), res.get('intervention_category'),
                            res.get('target_body_part'), res.get('specific_point'),
                            res.get('study_design'), res.get('clinical_score'),
                            res.get('summary'), p['title'], p['abstract'], 
                            res.get('icd_code'), "Abstract Saved"
                        ))
                        conn.commit()
                conn.close()
                st.success("ì €ì¥ ì™„ë£Œ!")
                st.session_state.search_res = None
                time.sleep(1)
                st.rerun()
                
                
# --- [Tab 5: ìë™í™” ì„¤ì • (NEW)] ---
with tab_settings:
    st.subheader("ğŸ¤– ìë™í™” ë´‡ ì œì–´íŒ")
    st.markdown("""
    ì—¬ê¸°ì„œ ì„¤ì •ì„ ë³€ê²½í•˜ë©´, ë§¤ì¼ ì•„ì¹¨ GitHub Actions ë´‡ì´ ì´ ì„¤ì •ì„ í™•ì¸í•˜ê³  ì‘ë™í•©ë‹ˆë‹¤.
    (ë‚´ ì»´í“¨í„°ê°€ êº¼ì ¸ ìˆì–´ë„ ëŒì•„ê°‘ë‹ˆë‹¤.)
    """)
    
    current_status = get_config("auto_bot_enabled")
    is_on = True if current_status == "True" else False
    
    st.divider()
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.markdown("#### ğŸš€ ìë™ ë¸Œë¦¬í•‘ ì „ì†¡")
        auto_toggle = st.toggle("ë§¤ì¼ ì•„ì¹¨ ìë™ ì‹¤í–‰ ì¼œê¸°", value=is_on)
        
        if auto_toggle != is_on:
            set_config("auto_bot_enabled", str(auto_toggle))
            st.success(f"ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {'ì¼œì§' if auto_toggle else 'êº¼ì§'}")
            time.sleep(1)
            st.rerun()
            
    with col2:
        st.info(f"""
        **í˜„ì¬ ìƒíƒœ:** {'ğŸŸ¢ ì‘ë™ ì¤‘' if auto_toggle else 'ğŸ”´ ì •ì§€ë¨'}
        
        **ì‘ë™ ì›ë¦¬:**
        1. ë§¤ì¼ ì•„ì¹¨ 8ì‹œ(KST)ì— ë´‡ì´ ê¹¨ì–´ë‚©ë‹ˆë‹¤.
        2. ì´ DB íŒŒì¼ì„ ì—´ì–´ì„œ **'ì¼œì§'** ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        3. ì¼œì ¸ ìˆë‹¤ë©´:
           - ì–´ì œ ë‚˜ì˜¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
           - Top 7 ë…¼ë¬¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ **ì´ DBì— ì €ì¥**í•©ë‹ˆë‹¤.
           - ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•´ **í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡**í•©ë‹ˆë‹¤.
           - **ì—…ë°ì´íŠ¸ëœ DBë¥¼ ìë™ìœ¼ë¡œ ì €ì¥ì†Œì— ë°±ì—…**í•©ë‹ˆë‹¤.
        """)
        
# (ë‚˜ë¨¸ì§€ ë©”ì¸ ì‹¤í–‰ ì½”ë“œ)
if __name__ == "__main__":
    init_db() # ì‹¤í–‰ ì‹œ DB ì´ˆê¸°í™”/ì—…ë°ì´íŠ¸